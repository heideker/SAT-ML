#!/usr/bin/env python3
# Author: Alexandre Heideker - FEI/UFABC - Brazil
# Date: 2025-november
# FAPESP - SMART Project - Clipping Sentinel-2 geotiffs (downloaded as ZIP files) to a given AOI GeoJSON.

# Features:
#     - AOI GeoJSON may contain MULTIPLE polygons.
#       Each polygon gets its own output tree.
#     - ALL Sentinel-2 bands are processed, regardless of resolution (10m, 20m, 60m).
#     - Dates and cloud cover are read from results.csv generated by download_s2.py:
#           - 'id'    : product UUID (ZIP name)
#           - 'start' : ISO datetime string
#           - 'cloud' : cloud cover percentage (float)
#     - Output directory structure:

#         <output>/<polygon_id>/<yyyymmdd>CL<XXX>/<band_res>/<product_id>.tif

#       where:
#         - polygon_id : derived from feature properties or auto poly_XX
#         - yyyymmdd   : date from results.csv (start column)
#         - XXX        : integer cloud cover with 3 digits (e.g. 007, 023, 100).
#                        If unknown, 999 is used.
#         - band_res   : decoded band name + '_' + resolution, e.g.:
#                        Blue_10m, NIR_10m, RedEdge1_20m, SWIR1_20m, CoastalAerosol_60m

#     - Additionally, a summary CSV is written at <output>/summary.csv
#       with one row per clipped raster.

# Usage example:
#     python clip_s2_to_aoi.py \
#         --aoi setores_wgs84.geojson \
#         --input ./2025_10 \
#         --output ./2025_10_clip

# Requirements:
#     pip install rasterio shapely pyproj pandas

import argparse
import json
import os
import re
import zipfile
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

import pandas as pd
import rasterio
from rasterio.mask import mask
from shapely.geometry import shape as shapely_shape, mapping as shapely_mapping
from shapely.ops import transform as shapely_transform
from pyproj import Transformer


# ---------------------------------------------------------------------
# Band decoding (code -> human readable name)
# ---------------------------------------------------------------------

BAND_NAME_MAP: Dict[str, str] = {
    "B01": "CoastalAerosol",
    "B02": "Blue",
    "B03": "Green",
    "B04": "Red",
    "B05": "RedEdge1",
    "B06": "RedEdge2",
    "B07": "RedEdge3",
    "B08": "NIR",
    "B8A": "NarrowNIR",
    "B09": "WaterVapor",
    "B10": "Cirrus",
    "B11": "SWIR1",
    "B12": "SWIR2",
}


def parse_band_and_res(filename: str) -> Optional[Tuple[str, str]]:
    # Parse band code (e.g. B02, B8A) and resolution (e.g. 10m, 20m, 60m)
    # from a Sentinel-2 JP2 filename.

    # Typical pattern:
    #     T22KGA_20251015T132229_B02_10m.jp2
    #     T22KGA_20251015T132229_B8A_20m.jp2

    # Returns:
    #     (band_code, res_str) or None if not matched.

    m = re.search(r"_B(?P<band>[0-9A-Z]{2})_(?P<res>\d{2}m)\.jp2$", filename)
    if not m:
        return None
    band = "B" + m.group("band")
    res = m.group("res")  # e.g. "10m"
    return band, res


def decode_band_res(band: str, res: str) -> str:
    # Convert band/res to a descriptive folder name, e.g.:
    #     B02 + 10m  -> "Blue_10m"
    #     B11 + 20m  -> "SWIR1_20m"
    #     B01 + 60m  -> "CoastalAerosol_60m"
    # If band not known, fall back to the band code itself + res.

    decoded = BAND_NAME_MAP.get(band, band)
    return f"{decoded}_{res}"


# ---------------------------------------------------------------------
# AOI loading / area calculation
# ---------------------------------------------------------------------


def compute_polygon_area_ha(geom) -> float:
    # Compute polygon area in hectares by reprojecting to a suitable UTM zone.

    centroid = geom.centroid
    lon, lat = centroid.x, centroid.y
    zone = int((lon + 180) // 6) + 1
    if lat >= 0:
        epsg = 32600 + zone  # WGS 84 / UTM (N)
    else:
        epsg = 32700 + zone  # WGS 84 / UTM (S)

    transformer = Transformer.from_crs("EPSG:4326", f"EPSG:{epsg}", always_xy=True)

    def _transform(x, y, z=None):
        return transformer.transform(x, y)

    geom_proj = shapely_transform(_transform, geom)
    area_m2 = geom_proj.area
    return area_m2 / 10_000.0  # hectares


def load_aoi_features(aoi_path: str) -> List[Tuple[str, object, float]]:
    # Load all AOI polygons from GeoJSON.
    # Returns a list of (poly_id, shapely_geometry, area_ha).
    # poly_id is derived from feature properties if possible:
    #     - 'id', 'ID', 'name', 'nome', 'Nome'
    # Otherwise, 'poly_01', 'poly_02', ...

    with open(aoi_path, "r", encoding="utf-8") as f:
        gj = json.load(f)

    features = []
    if gj.get("type") == "FeatureCollection":
        raw_features = gj["features"]
    elif gj.get("type") == "Feature":
        raw_features = [gj]
    else:
        raw_features = [{"type": "Feature", "properties": {}, "geometry": gj}]

    for idx, feat in enumerate(raw_features, start=1):
        props: Dict = feat.get("properties") or {}
        candidate_keys = ["id", "ID", "name", "nome", "Nome"]
        poly_id = None
        for key in candidate_keys:
            if key in props and props[key]:
                poly_id = str(props[key])
                break
        if not poly_id:
            poly_id = f"poly_{idx:02d}"

        geom = shapely_shape(feat["geometry"])
        area_ha = compute_polygon_area_ha(geom)
        features.append((poly_id, geom, area_ha))

    return features


# ---------------------------------------------------------------------
# Helper functions
# ---------------------------------------------------------------------


def unzip_safe(zip_path: Path, work_dir: Path) -> Path:
    # Unzip a Sentinel-2 product ZIP into work_dir.
    # Returns the path to the .SAFE directory.

    with zipfile.ZipFile(zip_path, "r") as zf:
        zf.extractall(work_dir)

    safe_dirs = list(work_dir.glob("*.SAFE"))
    if not safe_dirs:
        raise RuntimeError(f"No .SAFE folder found after unzipping {zip_path}")
    return safe_dirs[0]


def find_all_band_files(safe_dir: Path) -> List[Path]:
    # Find ALL band files (JP2) inside a Sentinel-2 SAFE directory.
    # We only consider files under 'IMG_DATA' folders, for any resolution
    # (R10m, R20m, R60m, or equivalent structure).

    jp2_files: List[Path] = []
    for p in safe_dir.rglob("*.jp2"):
        if "IMG_DATA" in str(p):
            jp2_files.append(p)
    return sorted(jp2_files)


def reproject_geometry(geom, src_crs: str, dst_crs) -> dict:

    # Reproject a shapely geometry from src_crs to dst_crs
    # and return as GeoJSON-like mapping.

    transformer = Transformer.from_crs(src_crs, dst_crs, always_xy=True)

    def _transform(x, y, z=None):
        return transformer.transform(x, y)

    geom_reproj = shapely_transform(_transform, geom)
    return shapely_mapping(geom_reproj)


def clip_raster_to_geom(
    src_path: Path,
    dst_path: Path,
    geom,
    aoi_crs="EPSG:4326",
):
    # Clip a single raster (src_path) to AOI geometry (geom in aoi_crs),
    # save result as dst_path (GeoTIFF).

    with rasterio.open(src_path) as src:
        src_crs = src.crs
        if src_crs is None:
            raise RuntimeError(f"Raster {src_path} has no CRS")

        geom_reproj = reproject_geometry(geom, src_crs=aoi_crs, dst_crs=src_crs)

        out_image, out_transform = mask(
            src,
            [geom_reproj],
            crop=True,
            nodata=src.nodata,
        )

        out_meta = src.meta.copy()
        out_meta.update(
            {
                "height": out_image.shape[1],
                "width": out_image.shape[2],
                "transform": out_transform,
            }
        )

        dst_path.parent.mkdir(parents=True, exist_ok=True)
        with rasterio.open(dst_path, "w", **out_meta) as dst:
            dst.write(out_image)


def format_cloud_percent(cloud: Optional[float]) -> str:
    # Format cloud percentage as 3-digit integer string (XXX).
    # Rules:
    #     - None or NaN -> '999' (unknown)
    #     - negative -> 000
    #     - above 999 -> 999 (safety cap)
    #     - otherwise: int(cloud) -> 3 digits, zero-padded
    if cloud is None:
        return "999"
    try:
        if pd.isna(cloud):
            return "999"
    except Exception:
        return "999"

    try:
        val = int(float(cloud))
    except Exception:
        return "999"

    if val < 0:
        val = 0
    if val > 999:
        val = 999
    return f"{val:03d}"


def load_metadata_from_results(
    input_dir: Path,
) -> Tuple[Dict[str, str], Dict[str, Optional[float]]]:
    # Load product dates and cloud cover from results.csv in input_dir.

    # Expects columns:
    #     - 'id'    : product UUID (ZIP filename stem)
    #     - 'start' : ISO datetime string, e.g. 2025-10-15T13:22:29.024000Z
    #     - 'cloud' : float percentage (may be NaN)

    # Returns:
    #     id_to_date  : dict product_id -> 'yyyymmdd'
    #     id_to_cloud : dict product_id -> cloud percentage (float or None)

    csv_path = input_dir / "results.csv"
    if not csv_path.exists():
        print(f"‚ö†Ô∏è  results.csv not found in {input_dir}, dates/clouds will be default.")
        return {}, {}

    df = pd.read_csv(csv_path)
    missing_cols = [c for c in ("id", "start", "cloud") if c not in df.columns]
    if missing_cols:
        print(
            f"‚ö†Ô∏è  results.csv missing columns {missing_cols}, "
            "dates/clouds will be partial/default."
        )
    id_to_date: Dict[str, str] = {}
    id_to_cloud: Dict[str, Optional[float]] = {}

    for _, row in df.iterrows():
        pid = str(row.get("id", "")).strip()
        if not pid:
            continue

        # Date
        start_str = str(row.get("start", ""))
        if len(start_str) >= 10:
            date_part = start_str[:10]  # 'YYYY-MM-DD'
            yyyymmdd = date_part.replace("-", "")
        else:
            yyyymmdd = "unknown_date"
        id_to_date[pid] = yyyymmdd

        # Cloud
        cloud_val = row.get("cloud", None)
        if isinstance(cloud_val, str) and not cloud_val.strip():
            cloud_val = None
        id_to_cloud[pid] = cloud_val

    print(f"üìÑ Loaded metadata for {len(id_to_date)} products from {csv_path}")
    return id_to_date, id_to_cloud


# ---------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Clip Sentinel-2 SAFE products (ZIP) to AOI GeoJSON.\n"
            "AOI may contain multiple polygons and ALL bands/resolutions are processed.\n"
            "Dates and cloud cover are read from results.csv.\n"
            "Output structure:\n"
            "  <output>/<polygon_id>/<yyyymmdd>CL<XXX>/<band_res>/<product_id>.tif\n"
            "Additionally writes <output>/summary.csv with one row per clipped raster."
        )
    )
    parser.add_argument(
        "--aoi",
        required=True,
        help="Path to AOI GeoJSON (may contain multiple polygons).",
    )
    parser.add_argument(
        "--input",
        required=True,
        help=(
            "Directory where Sentinel-2 ZIP products were downloaded "
            "and where results.csv is located."
        ),
    )
    parser.add_argument(
        "--output",
        required=True,
        help="Output directory for clipped GeoTIFFs and summary.csv.",
    )
    parser.add_argument(
        "--keep_unzipped",
        action="store_true",
        help="If set, do not delete unzipped SAFE directories in the working folder.",
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()

    input_dir = Path(args.input)
    output_dir = Path(args.output)
    work_dir = output_dir / "_unzipped"

    print(f"üìÇ Input ZIP directory   : {input_dir}")
    print(f"üìÇ Output clips directory: {output_dir}")
    print(f"üìÇ Working directory     : {work_dir}")

    # Load AOI features (assumed EPSG:4326)
    print(f"üîç Loading AOI polygons from: {args.aoi}")
    aoi_features = load_aoi_features(args.aoi)
    print(f"üß© Found {len(aoi_features)} polygons in AOI.")

    # Load dates and cloud mapping from results.csv
    id_to_date, id_to_cloud = load_metadata_from_results(input_dir)

    # Find all ZIP products
    zip_files = sorted(input_dir.glob("*.zip"))
    if not zip_files:
        print("No ZIP files found in input directory.")
        return

    print(f"üõ∞Ô∏è  Found {len(zip_files)} ZIP products to process.")

    summary_rows: List[Dict[str, Any]] = []

    for zip_path in zip_files:
        print(f"\n=== Processing product: {zip_path.name} ===")

        # product_id is the UUID = zip filename (without .zip)
        product_id = zip_path.stem
        date_str = id_to_date.get(product_id, "unknown_date")
        cloud_val = id_to_cloud.get(product_id, None)
        cloud_code = format_cloud_percent(cloud_val)

        # Cloud integer for summary (clamp to 0‚Äì100)
        cloud_int: Optional[int]
        if cloud_val is None or (isinstance(cloud_val, float) and pd.isna(cloud_val)):
            cloud_int = None
        else:
            try:
                c = int(float(cloud_val))
                if c < 0:
                    c = 0
                if c > 100:
                    c = 100
                cloud_int = c
            except Exception:
                cloud_int = None

        date_cloud_dir = f"{date_str}CL{cloud_code}"
        print(f"  Date from results.csv: {date_str}, cloud: {cloud_val} -> CL{cloud_code}")

        # 1) Unzip SAFE
        safe_work_dir = work_dir / product_id
        safe_work_dir.mkdir(parents=True, exist_ok=True)
        print(f"  Unzipping to: {safe_work_dir}")
        safe_dir = unzip_safe(zip_path, safe_work_dir)
        print(f"  SAFE directory: {safe_dir}")

        # 2) Find ALL bands under IMG_DATA
        band_files = find_all_band_files(safe_dir)
        print(f"  Found {len(band_files)} band files (*.jp2) under IMG_DATA")

        # 3) For each polygon, clip all bands
        for poly_id, poly_geom, poly_area_ha in aoi_features:
            print(f"  ‚ûú Clipping for polygon: {poly_id}")

            for band_path in band_files:
                band_info = parse_band_and_res(band_path.name)
                if not band_info:
                    # Skip files that do not follow the expected pattern
                    print(f"      (Skipping non-band file: {band_path.name})")
                    continue

                band_code, res_str = band_info
                band_res_name = decode_band_res(band_code, res_str)

                # Directory: <output>/<polygon_id>/<yyyymmdd>CL<XXX>/<band_res>/
                band_out_dir = output_dir / poly_id / date_cloud_dir / band_res_name
                band_out_dir.mkdir(parents=True, exist_ok=True)

                # File name: <product_id>.tif (1 file per product per band/polygon)
                dst_band_path = band_out_dir / f"{product_id}.tif"

                # Avoid reprocessing if already exists
                if dst_band_path.exists():
                    print(f"      ‚Ä¢ Skipping existing: {dst_band_path.relative_to(output_dir)}")
                    # Still record in summary if not yet added
                    summary_rows.append(
                        {
                            "polygon_id": poly_id,
                            "polygon_area_ha": poly_area_ha,
                            "date": date_str,
                            "cloud_percent": cloud_int,
                            "cloud_code": cloud_code,
                            "product_id": product_id,
                            "band_code": band_code,
                            "band_name": BAND_NAME_MAP.get(band_code, band_code),
                            "resolution_m": int(res_str.replace("m", "")),
                            "band_res_name": band_res_name,
                            "tif_path": str(dst_band_path.relative_to(output_dir)),
                        }
                    )
                    continue

                print(
                    f"      ‚Ä¢ {band_path.relative_to(safe_dir)} "
                    f"-> {dst_band_path.relative_to(output_dir)}"
                )
                clip_raster_to_geom(
                    src_path=band_path,
                    dst_path=dst_band_path,
                    geom=poly_geom,
                    aoi_crs="EPSG:4326",
                )

                # Add row to summary
                summary_rows.append(
                    {
                        "polygon_id": poly_id,
                        "polygon_area_ha": poly_area_ha,
                        "date": date_str,
                            "cloud_percent": cloud_int,
                        "cloud_code": cloud_code,
                        "product_id": product_id,
                        "band_code": band_code,
                        "band_name": BAND_NAME_MAP.get(band_code, band_code),
                        "resolution_m": int(res_str.replace("m", "")),
                        "band_res_name": band_res_name,
                        "tif_path": str(dst_band_path.relative_to(output_dir)),
                    }
                )

        # 4) Optionally clean unzipped SAFE
        if not args.keep_unzipped:
            print(f"  Removing temporary SAFE folder: {safe_work_dir}")
            for root, dirs, files in os.walk(safe_work_dir, topdown=False):
                for name in files:
                    os.remove(Path(root) / name)
                for name in dirs:
                    os.rmdir(Path(root) / name)
            os.rmdir(safe_work_dir)

    # -----------------------------------------------------------------
    # Write summary CSV
    # -----------------------------------------------------------------
    if summary_rows:
        summary_df = pd.DataFrame(summary_rows)
        summary_csv = output_dir / "summary.csv"
        summary_df.to_csv(summary_csv, index=False)
        print(f"\nüßæ Summary CSV written to: {summary_csv}")
    else:
        print("\n‚ö†Ô∏è No clipped rasters to summarize; summary.csv not created.")

    print("\n‚úÖ All done. Clipped rasters saved in:", output_dir)


if __name__ == "__main__":
    main()
